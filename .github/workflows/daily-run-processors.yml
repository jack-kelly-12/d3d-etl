name: Daily Processors

on:
  workflow_run:
    workflows: ["Daily Pull"]
    types: [completed]
  workflow_dispatch:

concurrency:
  group: d3d-etl-daily-processors
  cancel-in-progress: false

jobs:
  run-processors:
    if: github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 360
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: d3d-etl-processors
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure data dir exists
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
        run: |
          set -euo pipefail
          mkdir -p "$DATA_ROOT"

      - name: S3 pre-pull
        if: vars.S3_BUCKET != ''
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_PREFIX: ${{ vars.S3_PREFIX }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -euo pipefail
          python -m s3_utils.sync_data \
            --mode pull \
            --bucket "$S3_BUCKET" \
            --prefix "$S3_PREFIX" \
            --local_dir "$DATA_ROOT" \
            --region "$AWS_REGION"

      - name: Run processors
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
          YEARS: ${{ vars.YEARS }}
          DIVISIONS: ${{ vars.DIVISIONS }}
        run: |
          set -euo pipefail

          if [ -z "${YEARS:-}" ]; then
            echo "ERROR: vars.YEARS is empty (example: 2026)"
            exit 1
          fi
          if [ -z "${DIVISIONS:-}" ]; then
            echo "ERROR: vars.DIVISIONS is empty (example: 1,2,3)"
            exit 1
          fi

          IFS=',' read -r -a YEARS_ARR <<< "$YEARS"
          IFS=',' read -r -a DIVS_ARR <<< "$DIVISIONS"

          python -u -m processors.run_all \
            --data_dir "$DATA_ROOT" \
            --years "${YEARS_ARR[@]}" \
            --divisions "${DIVS_ARR[@]}"

      - name: Always upload processors outputs to S3
        if: always() && vars.S3_BUCKET != '' && vars.S3_UPLOAD_AFTER_RUN == 'true'
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_PREFIX: ${{ vars.S3_PREFIX }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_SYNC_DELETE: ${{ vars.S3_SYNC_DELETE }}
          S3_SSE_MODE: ${{ vars.S3_SSE_MODE }}
          S3_SSE_KMS_KEY_ID: ${{ vars.S3_SSE_KMS_KEY_ID }}
        run: |
          set -euo pipefail

          EXTRA_ARGS=()
          if [ "${S3_SYNC_DELETE:-false}" = "true" ]; then EXTRA_ARGS+=(--delete_extra); fi
          if [ -n "${S3_SSE_MODE:-}" ]; then EXTRA_ARGS+=(--sse_mode "$S3_SSE_MODE"); fi
          if [ -n "${S3_SSE_KMS_KEY_ID:-}" ]; then EXTRA_ARGS+=(--sse_kms_key_id "$S3_SSE_KMS_KEY_ID"); fi

          python -m s3_utils.sync_data \
            --mode push \
            --bucket "$S3_BUCKET" \
            --prefix "$S3_PREFIX" \
            --local_dir "$DATA_ROOT" \
            --region "$AWS_REGION" \
            "${EXTRA_ARGS[@]}"
