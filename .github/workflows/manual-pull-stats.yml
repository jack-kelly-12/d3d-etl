name: Manual Pull Stats

on:
  workflow_dispatch:

concurrency:
  group: d3d-etl-manual-stats
  cancel-in-progress: false

jobs:
  schedules-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install --with-deps chromium

      - name: Install Xvfb (headed Playwright support)
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: d3d-etl-manual-stats-schedules
          aws-region: ${{ secrets.AWS_REGION }}
          role-duration-seconds: 43200

      - name: Ensure output dirs exist
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
          SCHEDULES_OUTDIR: ${{ vars.SCHEDULES_OUTDIR }}
        run: |
          set -euo pipefail
          mkdir -p "$DATA_ROOT" "$SCHEDULES_OUTDIR"

      - name: Optional S3 pre-pull
        if: vars.S3_PULL_BEFORE_RUN == 'true' && vars.S3_BUCKET != ''
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_PREFIX: ${{ vars.S3_PREFIX }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -euo pipefail
          python -m s3_utils.sync_data \
            --mode pull \
            --bucket "$S3_BUCKET" \
            --prefix "$S3_PREFIX" \
            --local_dir "$DATA_ROOT" \
            --region "$AWS_REGION"

      - name: Run schedules scraper (headed via Xvfb)
        env:
          YEARS: ${{ vars.YEARS }}
          DIVISIONS: ${{ vars.DIVISIONS }}
          BASE_DELAY: ${{ vars.BASE_DELAY }}
          TEAM_IDS_FILE: ${{ vars.TEAM_IDS_FILE }}
          SCHEDULES_OUTDIR: ${{ vars.SCHEDULES_OUTDIR }}
        run: |
          set -euo pipefail

          if [ -z "${YEARS:-}" ]; then
            echo "ERROR: vars.YEARS is empty (example: 2026)"
            exit 1
          fi
          if [ -z "${DIVISIONS:-}" ]; then
            echo "ERROR: vars.DIVISIONS is empty (example: 1,2,3)"
            exit 1
          fi
          if [ -z "${TEAM_IDS_FILE:-}" ]; then
            echo "ERROR: vars.TEAM_IDS_FILE is empty"
            exit 1
          fi

          IFS=',' read -r -a YEARS_ARR <<< "$YEARS"
          IFS=',' read -r -a DIVS_ARR <<< "$DIVISIONS"

          for YEAR in "${YEARS_ARR[@]}"; do
            xvfb-run -a python -u -m scrapers.collect_schedules \
              --year "$YEAR" \
              --divisions "${DIVS_ARR[@]}" \
              --team_ids_file "$TEAM_IDS_FILE" \
              --outdir "$SCHEDULES_OUTDIR" \
              --base_delay "$BASE_DELAY"
          done

      - name: Upload schedules artifact for stats job
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ncaa-schedules
          path: ${{ vars.SCHEDULES_OUTDIR }}
          if-no-files-found: warn

      - name: Always upload schedules outputs to S3
        if: always() && vars.S3_BUCKET != '' && vars.S3_UPLOAD_AFTER_RUN == 'true'
        env:
          SCHEDULES_OUTDIR: ${{ vars.SCHEDULES_OUTDIR }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_PREFIX: ${{ vars.S3_PREFIX }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_SSE_MODE: ${{ vars.S3_SSE_MODE }}
          S3_SSE_KMS_KEY_ID: ${{ vars.S3_SSE_KMS_KEY_ID }}
        run: |
          set -euo pipefail

          EXTRA_ARGS=()
          if [ -n "${S3_SSE_MODE:-}" ]; then EXTRA_ARGS+=(--sse_mode "$S3_SSE_MODE"); fi
          if [ -n "${S3_SSE_KMS_KEY_ID:-}" ]; then EXTRA_ARGS+=(--sse_kms_key_id "$S3_SSE_KMS_KEY_ID"); fi

          python -m s3_utils.sync_data \
            --mode push \
            --bucket "$S3_BUCKET" \
            --prefix "${S3_PREFIX%/}/schedules" \
            --local_dir "$SCHEDULES_OUTDIR" \
            --region "$AWS_REGION" \
            "${EXTRA_ARGS[@]}"

  stats-scraper:
    needs: schedules-scraper
    runs-on: ubuntu-latest
    timeout-minutes: 360
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install --with-deps chromium

      - name: Install Xvfb (headed Playwright support)
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: d3d-etl-manual-stats-scraper
          aws-region: ${{ secrets.AWS_REGION }}
          role-duration-seconds: 43200

      - name: Ensure stats dirs exist
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
          SCHEDULES_OUTDIR: ${{ vars.SCHEDULES_OUTDIR }}
          STATS_OUTDIR: ${{ vars.STATS_OUTDIR }}
        run: |
          set -euo pipefail
          mkdir -p "$DATA_ROOT" "$SCHEDULES_OUTDIR" "$STATS_OUTDIR"

      - name: Optional S3 pre-pull
        if: vars.S3_PULL_BEFORE_RUN == 'true' && vars.S3_BUCKET != ''
        env:
          DATA_ROOT: ${{ vars.DATA_ROOT }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_PREFIX: ${{ vars.S3_PREFIX }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -euo pipefail
          python -m s3_utils.sync_data \
            --mode pull \
            --bucket "$S3_BUCKET" \
            --prefix "$S3_PREFIX" \
            --local_dir "$DATA_ROOT" \
            --region "$AWS_REGION"

      - name: Download schedules artifact
        uses: actions/download-artifact@v4
        with:
          name: ncaa-schedules
          path: ${{ vars.SCHEDULES_OUTDIR }}

      - name: Run stats scraper (headed via Xvfb)
        env:
          YEARS: ${{ vars.YEARS }}
          DIVISIONS: ${{ vars.DIVISIONS }}
          BASE_DELAY: ${{ vars.BASE_DELAY }}
          TEAM_IDS_FILE: ${{ vars.TEAM_IDS_FILE }}
          SCHEDULES_OUTDIR: ${{ vars.SCHEDULES_OUTDIR }}
          STATS_OUTDIR: ${{ vars.STATS_OUTDIR }}
        run: |
          set -euo pipefail

          if [ -z "${YEARS:-}" ]; then
            echo "ERROR: vars.YEARS is empty (example: 2026)"
            exit 1
          fi
          if [ -z "${DIVISIONS:-}" ]; then
            echo "ERROR: vars.DIVISIONS is empty (example: 1,2,3)"
            exit 1
          fi
          if [ -z "${TEAM_IDS_FILE:-}" ]; then
            echo "ERROR: vars.TEAM_IDS_FILE is empty"
            exit 1
          fi

          IFS=',' read -r -a YEARS_ARR <<< "$YEARS"
          IFS=',' read -r -a DIVS_ARR <<< "$DIVISIONS"

          for YEAR in "${YEARS_ARR[@]}"; do
            xvfb-run -a python -u -m scrapers.collect_stats \
              --year "$YEAR" \
              --divisions "${DIVS_ARR[@]}" \
              --team_ids_file "$TEAM_IDS_FILE" \
              --outdir "$STATS_OUTDIR" \
              --played_team_ids_dir "$SCHEDULES_OUTDIR" \
              --base_delay "$BASE_DELAY"
          done

      - name: Always upload stats outputs to S3
        if: always() && vars.S3_BUCKET != '' && vars.S3_UPLOAD_AFTER_RUN == 'true'
        env:
          STATS_OUTDIR: ${{ vars.STATS_OUTDIR }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_PREFIX: ${{ vars.S3_PREFIX }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_SSE_MODE: ${{ vars.S3_SSE_MODE }}
          S3_SSE_KMS_KEY_ID: ${{ vars.S3_SSE_KMS_KEY_ID }}
        run: |
          set -euo pipefail

          EXTRA_ARGS=()
          if [ -n "${S3_SSE_MODE:-}" ]; then EXTRA_ARGS+=(--sse_mode "$S3_SSE_MODE"); fi
          if [ -n "${S3_SSE_KMS_KEY_ID:-}" ]; then EXTRA_ARGS+=(--sse_kms_key_id "$S3_SSE_KMS_KEY_ID"); fi

          python -m s3_utils.sync_data \
            --mode push \
            --bucket "$S3_BUCKET" \
            --prefix "${S3_PREFIX%/}/stats" \
            --local_dir "$STATS_OUTDIR" \
            --region "$AWS_REGION" \
            "${EXTRA_ARGS[@]}"
